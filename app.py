# app.py
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import joblib
from pathlib import Path
import os
import plotly.express as px
import torch
import torch.nn as nn



# preprocessing_custom
from sklearn.base import BaseEstimator, TransformerMixin


def predict_from_X_scaled(model, y_scaler, X_scaled_2d: np.ndarray) -> float:
    """X_scaled_2d: shape (1, n_features)"""
    X_tensor = torch.tensor(X_scaled_2d.astype(np.float32))
    with torch.no_grad():
        y_scaled = model(X_tensor).cpu().numpy()  # (1,1)
    y = y_scaler.inverse_transform(y_scaled)[0, 0]
    return float(y)

def local_permutation_importance(
    model,
    y_scaler,
    X_scaled_2d: np.ndarray,
    feature_names: list[str],
    n_repeats: int = 10,
    mode: str = "shuffle",
    random_state: int = 42,
):
    """
    Importance locale sur un seul individu.
    - mode="shuffle": remplace la feature par une valeur tir√©e autour (bruit)
    - mode="zero": met la feature √† 0 (utile car donn√©es standardis√©es)
    Retourne un DataFrame tri√© d√©croissant.
    """
    rng = np.random.default_rng(random_state)

    base_pred = predict_from_X_scaled(model, y_scaler, X_scaled_2d)
    x0 = X_scaled_2d.copy()
    if x0.ndim != 2 or x0.shape[0] != 1 or x0.shape[1] == 0:
        raise ValueError(f"X_scaled_2d doit √™tre shape (1, n_features). Re√ßu {x0.shape}")


    importances = []
    for j in range(x0.shape[1]):
        deltas = []
        for _ in range(n_repeats):
            x_pert = x0.copy()

            if mode == "zero":
                x_pert[0, j] = 0.0
            else:
                # bruit gaussien autour de la valeur (ok en standardis√©)
                sigma = 1.0
                x_pert[0, j] = float(x0[0, j] + rng.normal(0.0, sigma))

            pred_pert = predict_from_X_scaled(model, y_scaler, x_pert)
            deltas.append(abs(pred_pert - base_pred))

        importances.append(float(np.mean(deltas)))

    df_imp = pd.DataFrame({
        "feature": feature_names,
        "impact_abs_moyen": importances
    }).sort_values("impact_abs_moyen", ascending=False).reset_index(drop=True)

    return base_pred, df_imp


def get_X_scaled_and_feature_names(preprocess, raw_features: dict):
    X_raw = pd.DataFrame([raw_features]).replace({None: np.nan})

    X_scaled = preprocess.transform(X_raw)

    # ‚úÖ si scipy sparse matrix -> densifier
    if hasattr(X_scaled, "toarray"):
        X_scaled = X_scaled.toarray()

    # ‚úÖ forcer un vrai ndarray float32 (pas object)
    X_scaled = np.asarray(X_scaled, dtype=np.float32)

    # ‚úÖ garantir shape (1, n_features)
    if X_scaled.ndim == 1:
        X_scaled = X_scaled.reshape(1, -1)

    # Noms des features finales (apr√®s OHE + num)
    feature_names = []
    try:
        ct = preprocess.named_steps["encode_and_scale"]
        feature_names = list(ct.get_feature_names_out())
    except Exception:
        feature_names = []

    # ‚úÖ fallback si pas de noms, ou mismatch longueur
    if (not feature_names) or (len(feature_names) != X_scaled.shape[1]):
        feature_names = [f"f{i}" for i in range(X_scaled.shape[1])]

    return X_scaled, feature_names




def safe_transform(preprocess, X_raw: pd.DataFrame):
    """
    Transform robuste:
    - tente preprocess.transform(X_raw)
    - si erreur 'columns are missing', on aligne sur feature_names_in_ puis on retente
    """
    try:
        return preprocess.transform(X_raw)
    except Exception as e:
        msg = str(e)
        if "columns are missing" in msg and hasattr(preprocess, "feature_names_in_"):
            expected = list(preprocess.feature_names_in_)
            X2 = X_raw.copy()
            for c in expected:
                if c not in X2.columns:
                    X2[c] = np.nan
            X2 = X2[expected]
            return preprocess.transform(X2)
        raise





class DropColumns(BaseEstimator, TransformerMixin):
    def __init__(self, cols=None):
        # cols peut √™tre None √† cause d'anciens pickles
        self.cols = list(cols) if cols is not None else []

    def __setstate__(self, state):
        """
        Appel√© par pickle/joblib au chargement.
        On r√©cup√®re les anciens noms d'attributs possibles.
        """
        self.__dict__.update(state)

        # r√©tro-compat : si l'ancien objet n'avait pas "cols"
        if not hasattr(self, "cols"):
            for alt in ("columns", "to_drop", "drop_cols", "cols_to_drop", "columns_to_drop"):
                if hasattr(self, alt):
                    self.cols = list(getattr(self, alt))
                    break
            else:
                self.cols = []

        # s√©curise le type
        if self.cols is None:
            self.cols = []
        if not isinstance(self.cols, (list, tuple, set)):
            self.cols = [self.cols]
        self.cols = list(self.cols)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()

        cols = getattr(self, "cols", None)
        if cols is None:
            cols = []

        # au cas o√π encore une autre version existait
        if len(cols) == 0:
            for alt in ("columns", "to_drop", "drop_cols", "cols_to_drop", "columns_to_drop"):
                if hasattr(self, alt) and getattr(self, alt) is not None:
                    cols = list(getattr(self, alt))
                    break

        return X.drop(columns=[c for c in cols if c in X.columns], errors="ignore")


class OrdinalMapping(BaseEstimator, TransformerMixin):
    def __init__(self, cols=None, mapping=None, dropna=False):
        self.cols = list(cols) if cols is not None else []
        self.mapping = dict(mapping) if mapping is not None else {}
        self.dropna = dropna

    def __setstate__(self, state):
        self.__dict__.update(state)

        # --- r√©tro-compat pour cols ---
        if not hasattr(self, "cols"):
            for alt in ("columns", "col_oe", "ordinal_cols", "cols_to_map", "features"):
                if hasattr(self, alt):
                    self.cols = list(getattr(self, alt))
                    break
            else:
                self.cols = []

        # --- r√©tro-compat pour mapping ---
        if not hasattr(self, "mapping"):
            for alt in ("map", "mapper", "mapping_dict", "mappings"):
                if hasattr(self, alt):
                    self.mapping = dict(getattr(self, alt))
                    break
            else:
                self.mapping = {}

        # --- r√©tro-compat pour dropna ---
        if not hasattr(self, "dropna"):
            for alt in ("drop_na", "drop_nan", "drop_missing"):
                if hasattr(self, alt):
                    self.dropna = bool(getattr(self, alt))
                    break
            else:
                self.dropna = False

        # s√©curise types
        if self.cols is None:
            self.cols = []
        if not isinstance(self.cols, (list, tuple, set)):
            self.cols = [self.cols]
        self.cols = list(self.cols)

        if self.mapping is None:
            self.mapping = {}
        if not isinstance(self.mapping, dict):
            self.mapping = dict(self.mapping)

        self.dropna = bool(self.dropna)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X = X.copy()

        cols = getattr(self, "cols", []) or []
        mapping = getattr(self, "mapping", {}) or {}
        dropna = bool(getattr(self, "dropna", False))

        for c in cols:
            if c in X.columns:
                X[c] = X[c].map(mapping)
                # on laisse les NaN -> imput√©s par la branche "num" ensuite
                if dropna:
                    X = X.dropna(subset=[c])

        return X


class RareCategoryGrouper(BaseEstimator, TransformerMixin):
    def __init__(self, cols=None, threshold=0.07, other_label="Autres"):
        self.cols = list(cols) if cols is not None else []
        self.threshold = float(threshold)
        self.other_label = other_label
        self.keep_values_ = {}

    def __setstate__(self, state):
        self.__dict__.update(state)

        if not hasattr(self, "cols"):
            for alt in ("columns", "cat_cols", "cols_to_group", "features"):
                if hasattr(self, alt):
                    self.cols = list(getattr(self, alt))
                    break
            else:
                self.cols = []

        if not hasattr(self, "threshold"):
            self.threshold = 0.07
        if not hasattr(self, "other_label"):
            self.other_label = "Autres"
        if not hasattr(self, "keep_values_"):
            self.keep_values_ = {}

        if self.cols is None:
            self.cols = []
        if not isinstance(self.cols, (list, tuple, set)):
            self.cols = [self.cols]
        self.cols = list(self.cols)

        self.threshold = float(self.threshold)

    def fit(self, X, y=None):
        X = X.copy()
        self.keep_values_ = {}
        n = len(X)
        for c in self.cols:
            if c not in X.columns:
                continue
            vc = X[c].fillna("Vide").value_counts(dropna=False)
            freq = vc / max(n, 1)
            self.keep_values_[c] = set(freq[freq >= self.threshold].index.tolist())
        return self

    def transform(self, X):
        X = X.copy()
        for c in self.cols:
            if c not in X.columns:
                continue
            keep = self.keep_values_.get(c, set())
            s = X[c].fillna("Vide")
            X[c] = s.where(s.isin(keep), other=self.other_label)
        return X



# ----------------------------
# CONFIG
# ----------------------------
st.set_page_config(
    page_title="Pr√©diction DPE - Projet datascience",
    page_icon="üè†",
    layout="wide",
)


# ----------------------------
# UTILS: chargements en cache
# ----------------------------
@st.cache_data(show_spinner=False)
def load_viz_data(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    return df

@st.cache_resource(show_spinner=False)
def load_model(path: Path):
    # id√©alement: un Pipeline sklearn qui inclut preprocessing + mod√®le
    return joblib.load(path)

# ----------------------------
# UI: Sidebar navigation
# ----------------------------
st.sidebar.title("Menu")
page = st.sidebar.radio(
    "√âtapes :",
    [
        "üèÅ Pr√©sentation",
        "üìä Dataviz",
        "üìà R√©sultats d'entra√Ænement",
        "üßÆ Pr√©diction DPE",
    ],
)

st.sidebar.markdown("---")
st.sidebar.caption("Projet ML - Pr√©diction DPE")

# ----------------------------
# PAGE 1: Pr√©sentation
# ----------------------------
def page_presentation():
    # --- Sidebar : L'√©quipe ---
    with st.sidebar:
        st.markdown("### üë• L'√âquipe")
        st.markdown("""
        * **Yacine Bennouna**
        * **Aymane Karani**
        * **Dylan Nefnaf**
        * **Guillaume Deschamps**
        """)
        st.divider()
        st.info("Projet dans le cadre du cursus Datascientist de Datascientest")

    # --- En-t√™te Principal ---
    st.title("üè° Projet DPE : Mod√©lisation & Pr√©diction")
    
    st.markdown("""
    **Bienvenue sur l'interface de restitution de notre projet.**
    
    Ce projet explore les donn√©es du *Diagnostic de Performance √ânerg√©tique (DPE)* en France. 
    Il vise √† appliquer des mod√®les de Machine Learning pour pr√©dire l'√©tiquette √©nerg√©tique des logements 
    et comprendre les facteurs d√©terminants de la consommation, √† la crois√©e des enjeux techniques, √©conomiques et scientifiques.
    """)

    st.divider()

    # --- Organisation en Onglets ---
    tab_contexte, tab_objectifs, tab_donnees = st.tabs(["üåç Contexte & R√©forme", "üéØ Objectifs", "üíæ Donn√©es ADEME"])

    # --- ONGLET 1 : CONTEXTE ---
    with tab_contexte:
        st.header("Contexte R√©glementaire et Technique")
        
        st.markdown("""
        Le DPE a subi une **r√©forme majeure le 1er juillet 2021** pour devenir un outil opposable et plus fiable. 
        Notre projet s'appuie exclusivement sur les donn√©es issues de ce nouveau cadre.
        """)

        with st.expander("‚öñÔ∏è La R√©forme DPE 2021 (Ce qui change)", expanded=True):
            st.markdown("""
            * **M√©thode de calcul unifi√©e (3CL) :** Fin de la m√©thode "sur facture". Le calcul est d√©sormais standardis√© pour tous les logements.
            * **Double Seuil :** L'√©tiquette (A √† G) est d√©termin√©e par la plus mauvaise note entre la **consommation d'√©nergie** et les **√©missions de gaz √† effet de serre (GES)**.
            * **5 Usages :** Prise en compte de l'√©clairage et des auxiliaires (en plus du chauffage, de l'eau chaude et du refroidissement).
            """)

        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown("#### üõ†Ô∏è Enjeu Technique")
            st.markdown("""
            La complexit√© r√©side dans la reproduction d'une m√©thode r√©glementaire stricte par des mod√®les statistiques.
            Le d√©fi est de g√©rer des donn√©es h√©t√©rog√®nes (mat√©riaux, syst√®mes) et de pr√©dire une classe d√©finie par des r√®gles physiques.
            """)
        
        with col_b:
            st.markdown("#### üí∞ Enjeu √âconomique")
            st.markdown("""
            Le DPE conditionne la valeur v√©nale ("Valeur Verte") et locative.
            L'objectif est d'aider √† la d√©cision pour prioriser les r√©novations et anticiper les interdictions de location (passoires thermiques G+ d√®s 2023, G en 2025).
            """)

    # --- ONGLET 2 : OBJECTIFS ---
    with tab_objectifs:
        st.header("Objectifs du Projet")
        
        col1, col2, col3 = st.columns(3)

        with col1:
            st.info("ü§ñ **Mod√©lisation ML**")
            st.markdown("""
            * **Classification :** Pr√©dire l'√©tiquette DPE (7 classes).
            * **R√©gression :** Estimer la consommation en kWh/m¬≤/an.
            * **Comparaison :** Random Forest vs XGBoost vs Deep Learning.
            """)

        with col2:
            st.warning("üìä **Analyse & Biais**")
            st.markdown("""
            * **Facteurs cl√©s :** Identifier les variables les plus influentes (Feature Importance).
            * **D√©s√©quilibre :** G√©rer la sous-repr√©sentation des classes extr√™mes (A et G).
            * **Simplification :** Tester l'impact de la r√©duction des variables.
            """)

        with col3:
            st.success("üß† **Interpr√©tabilit√©**")
            st.markdown("""
            * **Explicabilit√© :** Utiliser SHAP pour comprendre les d√©cisions du mod√®le.
            * **Critique :** √âvaluer la capacit√© du ML √† approximer une r√©glementation.
            * **Outil m√©tier :** Proposer un simulateur interactif.
            """)

    # --- ONGLET 3 : DONN√âES (ENRICHI) ---
    with tab_donnees:
        st.header("Le Jeu de Donn√©es ADEME")
        
        # M√©triques mises √† jour avec les chiffres officiels
        col_m1, col_m2, col_m3 = st.columns(3)
        col_m1.metric("Volume Total", "~13.6 Millions (12M au d√©but du projet)", "DPE (Recensement continu)")
        col_m2.metric("Fr√©quence", "Hebdomadaire", "Mise √† jour")
        col_m3.metric("P√©rim√®tre", "France", "Logements Existants")

        st.markdown("---")
        
        st.markdown("### üîç Sp√©cificit√©s du Dataset")
        st.warning("""
        **‚ö†Ô∏è Attention aux biais d'interpr√©tation :**
        Selon l'ADEME, cette base n'est **pas repr√©sentative de l'ensemble du parc immobilier fran√ßais**.
        Elle ne contient que les DPE r√©alis√©s obligatoirement lors de **ventes, locations ou constructions neuves**. 
        Un redressement statistique (croisement avec donn√©es INSEE) serait n√©cessaire pour une extrapolation nationale parfaite.
        """)

        st.markdown("""
        * **Source :** Base officielle [DPE Logements existants (depuis juillet 2021)](https://data.ademe.fr/datasets/dpe03existant).
        * **Contenu :** Caract√©ristiques techniques (b√¢ti, isolation, chauffage), consommations √©nerg√©tiques et √©missions GES.
        * **Filtres appliqu√©s pour le projet :**
            * Logements r√©sidentiels uniquement (Maisons & Appartements).
            * Donn√©es nettoy√©es des valeurs aberrantes et doublons.
        """)
        
        st.caption("Donn√©es sous Licence Ouverte / Open Licence version 2.0 - Producteur : ADEME")
        
        
        
# ----------------------------
# PAGE 2: Dataviz
# ----------------------------

def display_img(filename, caption=""):
    """Fonction utilitaire pour g√©rer l'affichage s√©curis√© des images"""
    path = f"img/{filename}"
    if os.path.exists(path):
        st.image(path, caption=caption, use_container_width=True)
    else:
        st.warning(f"‚ö†Ô∏è Image manquante : {path}")

def page_dataviz():
    st.title("üìä Visualisation des Donn√©es DPE")
    st.markdown("""
    Cette section explore la r√©partition des classes √©nerg√©tiques en France et analyse les corr√©lations 
    avec les caract√©ristiques physiques et g√©ographiques des logements.
    """)

    # Cr√©ation d'onglets pour organiser la navigation
    tab1, tab2, tab3, tab4 = st.tabs([
        "üåç Panorama National", 
        "üó∫Ô∏è G√©ographie & Climat", 
        "üèóÔ∏è Caract√©ristiques B√¢ti", 
        "‚è≥ P√©riode construction & Surface"
    ])

    # --- ONGLET 1 : PANORAMA NATIONAL ---
    with tab1:
        st.header("√âtat des lieux du parc immobilier")
        
        st.markdown("### 1. R√©partition DPE & GES")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**√âtiquette √ânergie (DPE)**")
            display_img("repartition_etiquette_DPE_France.png", "R√©partition nationale des DPE")
        with col2:
            st.markdown("**√âtiquette Climat (GES)**")
            display_img("repartition_etiquette_GES_France.png", "R√©partition nationale des GES")
            
        st.info("üí° **Note :** On observe souvent une corr√©lation entre les √©tiquettes DPE et GES, bien que le mode de chauffage influence fortement le GES.")

        st.markdown("### 2. Consommation r√©elle")
        display_img("repartition_conso_France.png", "Distribution de la consommation √©nerg√©tique (kWh/m¬≤/an)")

    # --- ONGLET 2 : G√âOGRAPHIE ---
    with tab2:
        st.header("Disparit√©s Territoriales")
        
        st.markdown("### 1. La France des passoires vs b√¢timents √©colos")
        c1, c2 = st.columns(2)
        with c1:
            display_img("part_passoires_thermiques_par_departement.png", "Part des passoires (F & G)")
        with c2:
            display_img("part_batiments_ecolo_par_departements.png", "Part des b√¢timents performants (A & B)")

        st.markdown("---")
        
        st.markdown("### 2. Influence de l'environnement")
        c3, c4 = st.columns(2)
        with c3:
            st.subheader("Par R√©gion")
            display_img("repartition_DPE_regions.png", "DPE par R√©gion administrative")
        with c4:
            st.subheader("Par Zone Climatique")
            display_img("repartition_zone_climatique.png", "Impact du climat local")
            
        st.markdown("#### Focus Altitude")
        display_img("repartition_classe_altitude.png", "R√©partition des classes selon l'altitude")

    # --- ONGLET 3 : CARACT√âRISTIQUES B√ÇTI ---
    with tab3:
        st.header("Impact technique sur la performance")

        st.markdown("### 1. Type de b√¢timent & √ânergie")
        # Comparaison Maison vs Appartement (DPE & GES)
        c1, c2 = st.columns(2)
        with c1:
            display_img("etiquette_DPE_type_bat.png", "DPE selon le type de logement")
        with c2:
            display_img("etiquette_GES_type_bat.png", "GES selon le type de logement")
            
        st.markdown("#### Source d'√©nergie principale")
        display_img("repartition_type_energie_n1.png", "R√©partition par type d'√©nergie")

        st.markdown("---")
        st.markdown("### 2. Inertie du b√¢timent")
        st.markdown("L'inertie thermique joue un r√¥le cl√© dans le confort et la performance.")
        display_img("repartition_classe_inertie_batiment.png", "Classement selon l'inertie")

    # --- ONGLET 4 : ann√©e construction ET SURFACE ---
    with tab4:
        st.header("Construction et Dimensions")

        st.markdown("### 1. L'impact de l'anciennet√©")
        st.markdown("L'√©volution des normes de construction au fil du temps :")
        
        c1, c2 = st.columns(2)
        with c1:
            display_img("repartition_etiquette_periode.png", "√âtiquettes par p√©riode de construction")
        with c2:
            display_img("repartition_periode_etiquette.png", "P√©riodes de construction par √©tiquette")

        st.markdown("---")

        st.markdown("### 2. L'impact de la surface")
        st.markdown("Les petites surfaces sont-elles d√©favoris√©es par le calcul du DPE ?")
        
        display_img("surface_etiquette_boxplot.png", "Distribution des surfaces par √©tiquette")

        with st.expander("üîé D√©tail du nettoyage des donn√©es (Outliers)"):
            st.write("Analyse de la distribution des surfaces avant et apr√®s traitement des valeurs aberrantes.")
            col_a, col_b = st.columns(2)
            with col_a:
                display_img("surface_without_outliers.png", "Surface sans outliers")
            with col_b:
                display_img("surface_without_outliers_dist.png", "Distribution nettoy√©e")               
                
# ----------------------------
# PAGE 3: R√©sultats d'entra√Ænement
# ----------------------------
def page_results():
    st.title("ü§ñ Mod√©lisation & R√©sultats")
    st.markdown("""
    Nous avons test√© deux approches pour pr√©dire la performance √©nerg√©tique :
    1.  **Classification** : Pr√©dire l'√©tiquette DPE (A √† G).
    2.  **R√©gression** : Pr√©dire la consommation d'√©nergie primaire ($kWh/m^2/an$).
    
    *Contrainte : Utilisation d'une baseline √† 16 colonnes pour g√©rer la charge m√©moire.*
    """)

    tab_classif, tab_reg = st.tabs(["üî† Approche Classification", "üìà Approche R√©gression"])

    # --- ONGLET 1 : CLASSIFICATION ---
    with tab_classif:
        st.header("Classification des √©tiquettes DPE")
        st.markdown("Objectif : Pr√©dire la classe exacte (A, B, C, D, E, F, G).")

        # 1. Comparaison Baseline
        st.subheader("1. Benchmark des mod√®les (Baseline)")
        data_classif = {
            "Mod√®le": ["Random Forest", "KNN", "Decision Tree", "Logistic Regression", "Naive Bayes"],
            "Accuracy": [0.577, 0.546, 0.526, 0.526, 0.031],
            "F1-Score": [0.564, 0.539, 0.510, 0.510, 0.024]
        }
        df_classif = pd.DataFrame(data_classif).sort_values(by="Accuracy", ascending=False)
        
        col1, col2 = st.columns([1, 2])
        with col1:
            st.dataframe(df_classif.style.highlight_max(axis=0, color="#d1e7dd"), use_container_width=True)
        with col2:
            fig_classif = px.bar(df_classif, x="Accuracy", y="Mod√®le", orientation='h', 
                                 title="Pr√©cision par mod√®le (Baseline)", color="Accuracy", color_continuous_scale="Viridis")
            st.plotly_chart(fig_classif, use_container_width=True)

        # 2. Focus Meilleur Mod√®le
        st.subheader("2. Meilleur Mod√®le : Random Forest Optimis√©")
        st.markdown("Apr√®s optimisation des hyperparam√®tres (GridSearch), les gains sont marginaux, sugg√©rant une limite intrins√®que aux donn√©es d'entr√©e.")

        met1, met2, met3 = st.columns(3)
        met1.metric("Accuracy Test", "58.3%", delta="+0.6% vs Baseline")
        met2.metric("F1-Score Weighted", "0.575")
        met3.metric("Meilleur params", "500 arbres, Max Depth 20")

        # Analyse des erreurs
        with st.expander("üîé Analyse d√©taill√©e (Matrice de Confusion & Rapport)"):
            st.markdown("#### Pourquoi plafonne-t-on √† 58% ?")
            st.markdown("""
            L'analyse de la matrice de confusion montre que les erreurs sont principalement **"√† une classe pr√®s"** :
            * Le mod√®le confond souvent **C et D** (les classes majoritaires).
            * Difficult√© sur les extr√™mes (A/B et F/G) √† cause du d√©s√©quilibre de classe.
            """)
            
            st.markdown("#### Rapport de Classification (Optimis√©)")
            report_data = {
                "Classe": ["A", "B", "C", "D", "E", "F", "G"],
                "Precision": [0.65, 0.60, 0.72, 0.56, 0.46, 0.39, 0.53],
                "Recall": [0.52, 0.33, 0.72, 0.65, 0.48, 0.18, 0.52],
                "F1-Score": [0.58, 0.43, 0.72, 0.60, 0.47, 0.25, 0.52]
            }
            st.dataframe(pd.DataFrame(report_data).set_index("Classe").style.background_gradient(cmap="Reds", subset=["F1-Score"]))
                    st.subheader("3. Interpr√©tabilit√© (SHAP)")

        st.markdown("""
        **Les classes A √† G correspondent aux √©tiquettes DPE √©nergie.**

        **Lecture d‚Äôun beeswarm SHAP :**
        - **Couleur** : valeur de la variable (bleu = faible, rouge = √©lev√©e)
        - **Axe horizontal** : impact sur la pr√©diction
          - √† droite : pousse vers une √©tiquette **plus d√©grad√©e**
          - √† gauche : pousse vers une √©tiquette **meilleure**
        - **Dispersion verticale** : variabilit√© de l‚Äôeffet dans le jeu de donn√©es
        """)

        # Figures SHAP pr√©-calcul√©es (recommand√© : stable et l√©ger)
        col_a, col_b = st.columns(2)

        with col_a:
            display_img("shap_global_bar.png", "SHAP global ‚Äî importance (Top 20)")
        with col_b:
            display_img("shap_global_beeswarm.png", "SHAP global- top features")

        with st.expander("D√©tail par classe (exemples A / D / G)"):
            display_img("shap_class_A_beeswarm.png", "SHAP ‚Äî classe A")
            display_img("shap_class_D_beeswarm.png", "SHAP ‚Äî classe D")
            display_img("shap_class_G_beeswarm.png", "SHAP ‚Äî classe G")

        with st.expander("Exemple d'explicabilit√© locale (waterfall)"):
            display_img("shap_local_waterfall_ex1.png", "SHAP local ‚Äî waterfall (exemple)")


            st.markdown("#### Rapport de Classification (Optimis√©)")
            st.markdown("""
            Le rapport de classification permet de comparer, pour chaque √©tiquette (A √† G), la pr√©cision, le rappel et le F1-score.
            On observe g√©n√©ralement une meilleure performance sur les classes centrales (C/D/E) et une difficult√© accrue sur les classes extr√™mes (A/B et F/G).
            """)

            # Matrice de confusion normalis√©e (image export√©e depuis le notebook)
            st.markdown("#### Matrice de confusion normalis√©e")
            display_img("confusion_matrix_norm.png", "Matrice de confusion normalis√©e (par classe r√©elle)")

            # Pires confusions (table ou figure export√©e depuis le notebook)
            st.markdown("#### Principales confusions du mod√®le")
            display_img("top_errors.png", "Top confusions (vrai vs pr√©dit)")

            # Performance par classe (barplot export√© depuis le notebook)
            st.markdown("#### Performance par classe")
            display_img("perf_par_classe.png", "Pr√©cision / rappel / F1-score par classe")
        st.subheader("3. Interpr√©tabilit√© (SHAP)")

        st.markdown("""
        **Les classes A √† G correspondent aux √©tiquettes DPE √©nergie.**

        **Lecture d‚Äôun beeswarm SHAP :**
        - **Couleur** : valeur de la variable (bleu = faible, rouge = √©lev√©e)
        - **Position horizontale** : impact sur la pr√©diction  
          - √† droite : pousse vers une √©tiquette **plus d√©grad√©e**
          - √† gauche : pousse vers une √©tiquette **meilleure**
        - **Dispersion verticale** : variabilit√© de l‚Äôeffet selon les logements

        Les variables attendues ‚Äúm√©tier‚Äù (surface, p√©riode de construction, isolation, √©nergie/syst√®me de chauffage) ressortent de mani√®re coh√©rente,
        ce qui renforce la cr√©dibilit√© de l‚Äôapproche.
        """)

        col_a, col_b = st.columns(2)
        with col_a:
            display_img("shap_global_bar.png", "SHAP global ‚Äî importance (Top 20)")
        with col_b:
            display_img("shap_global_beeswarm.png", "SHAP global ‚Äî beeswarm")

        with st.expander("D√©tail par classe (exemples A / D / G)"):
            display_img("shap_class_A_beeswarm.png", "SHAP ‚Äî classe A")
            display_img("shap_class_D_beeswarm.png", "SHAP ‚Äî classe D")
            display_img("shap_class_G_beeswarm.png", "SHAP ‚Äî classe G")

        with st.expander("Exemple d'explicabilit√© locale (waterfall)"):
            display_img("shap_local_waterfall_ex1.png", "SHAP local ‚Äî waterfall (exemple)")

    
    # --- ONGLET 2 : REGRESSION ---
    with tab_reg:
        st.header("Estimation de la consommation √©nerg√©tique")
        st.markdown("Objectif : Pr√©dire une valeur continue (kWh/m¬≤/an).")

        # 1. Benchmark ML Classique
        st.subheader("1. Benchmark Machine Learning")
        data_reg = {
            "Mod√®le": ["Random Forest", "KNN Regressor", "Lasso/Ridge/Linear", "Decision Tree"],
            "MAE": [44.75, 47.86, 54.79, 59.56],
            "R¬≤": [0.645, 0.576, 0.491, 0.424]
        }
        df_reg = pd.DataFrame(data_reg).sort_values(by="R¬≤", ascending=False)
        
        st.dataframe(df_reg.style.highlight_max(subset=["R¬≤"], color="#d1e7dd").highlight_min(subset=["MAE"], color="#d1e7dd"), use_container_width=True)
        st.caption("Le Random Forest domine largement les mod√®les lin√©aires classiques.")

        st.divider()

        # 2. Deep Learning vs Random Forest
        st.subheader("2. Le saut de performance : Deep Learning")
        st.markdown("""
        Nous avons entra√Æn√© un r√©seau de neurones avec plus de colonnes en entr√©e. 
        C'est l'approche qui donne les **meilleurs r√©sultats globaux**.
        """)

        col_res1, col_res2, col_res3 = st.columns(3)
        col_res1.metric("MAE (Erreur Moyenne)", "36.6 kWh/m¬≤", delta="-7 kWh vs RF", delta_color="normal")
        col_res2.metric("RMSE", "49.6")
        col_res3.metric("R¬≤ (Score)", "0.69", delta="+0.05 vs RF")

        # 3. Image d'analyse Deep Learning
        st.markdown("#### Analyse de l'entra√Ænement (Validation Loss)")
        st.markdown("Comparaison de la convergence selon la taille du batch (Batch Size).")
        
        # Affichage de l'image fournie
        try:
            st.image("img/loss_batch_size.png", caption="Comparaison du Val Loss par Batch Size", use_container_width=True)
            st.info("On remarque qu'un Batch Size plus grand (8192 - courbe verte) converge plus vite et offre une courbe plus stable.")
        except:
            st.warning("‚ö†Ô∏è Image 'img/loss_batch_size.png' introuvable.")



# ----------------------------
# PAGE 4: simulation avec le mod√®le entrain√©
# ----------------------------

# ----------------------------
# MODEL DEFINITION (identique √† l'entra√Ænement)
# ----------------------------
class MLPRegressorBN(nn.Module):
    def __init__(self, n_features, hidden_sizes=(256, 128, 64), dropout=0.1):
        super().__init__()
        layers = []
        in_dim = n_features

        for h in hidden_sizes:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.BatchNorm1d(h))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            in_dim = h

        layers.append(nn.Linear(in_dim, 1))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


# ----------------------------
# LOGIQUE METIER (classe DPE)
# ----------------------------
def get_classe_dpe(conso, ges):
    seuils = {
        "A": [70, 6],
        "B": [110, 11],
        "C": [180, 30],
        "D": [250, 50],
        "E": [330, 70],
        "F": [420, 100],
        "G": [float("inf"), float("inf")],
    }

    def get_letter(val, idx):
        for letter, limits in seuils.items():
            if val < limits[idx]:
                return letter
        return "G"

    letter_c = get_letter(conso, 0)
    letter_g = get_letter(ges, 1)

    order = "ABCDEFG"
    return letter_c if order.index(letter_c) > order.index(letter_g) else letter_g


# ----------------------------
# CHARGEMENT DES ARTEFACTS
# ----------------------------
@st.cache_resource
def load_artifacts(artifact_dir: str):
    artifact_dir = Path(artifact_dir)

    preprocess = joblib.load(artifact_dir / "preprocess.joblib")
    y_scaler = joblib.load(artifact_dir / "y_scaler.joblib")

    ckpt = torch.load(artifact_dir / "model.pt", map_location="cpu")
    cfg = ckpt.get("model_config", {})

    n_features = int(cfg.get("n_features"))
    hidden_sizes = tuple(cfg.get("hidden_sizes", (256, 128, 64)))
    dropout = float(cfg.get("dropout", 0.1))

    model = MLPRegressorBN(
        n_features=n_features,
        hidden_sizes=hidden_sizes,
        dropout=dropout,
    )
    model.load_state_dict(ckpt["state_dict"])
    model.eval()

    return preprocess, y_scaler, model


# r√©cup√©rer la pr√©diction √† partir du mod√®le : idem conso ou ges :
def predict_from_model(preprocess, y_scaler, model, raw_features: dict) -> float:
    """
    raw_features : dict avec les COLONNES BRUTES (avant preprocess)
    """
    X_raw = pd.DataFrame([raw_features])

    # Important: √©viter les NaN c√¥t√© cat (m√™me si preprocess impute)
    # (Le SimpleImputer cat remplace NaN par "Vide", donc ok, mais on s√©curise)
    X_raw = X_raw.replace({None: np.nan})

    # 1) preprocess -> matrice num√©rique
    X_scaled = preprocess.transform(X_raw)

    # 2) torch
    X_tensor = torch.tensor(np.asarray(X_scaled, dtype=np.float32))

    with torch.no_grad():
        y_scaled_pred = model(X_tensor).cpu().numpy()  # shape (1,1)

    # 3) inverse scaling -> conso r√©elle
    y_pred = y_scaler.inverse_transform(y_scaled_pred)[0, 0]
    return float(max(0.0, y_pred))


# ----------------------------
# PAGE STREAMLIT
# ----------------------------
def page_simulator():
    st.title("üèóÔ∏è Simulateur de DPE")

    ARTIFACT_DIR_CONSO = "models/20251228-conso"
    ARTIFACT_DIR_GES = "models/20251228-ges"

    try:
        preprocess_conso, y_scaler_conso, model_conso = load_artifacts(ARTIFACT_DIR_CONSO)
    except Exception as e:
        st.error(
            f"Impossible de charger les artefacts depuis `{ARTIFACT_DIR_CONSO}`.\n\n"
            f"Attendus : preprocess.joblib, y_scaler.joblib, model.pt\n\n"
            f"D√©tail : {e}"
        )
        st.stop()
    
    try:
        preprocess_ges, y_scaler_ges, model_ges = load_artifacts(ARTIFACT_DIR_GES)
    except Exception as e:
        st.error(
            f"Impossible de charger les artefacts depuis `{ARTIFACT_DIR_GES}`.\n\n"
            f"Attendus : preprocess.joblib, y_scaler.joblib, model.pt\n\n"
            f"D√©tail : {e}"
        )
        st.stop()

    # ‚úÖ cat√©gories issues de du training
    form_options = {
        "classe_altitude": [ "inf√©rieur √† 400m", "400-800m","sup√©rieur √† 800m"],
        "periode_construction": [
            "avant 1948", "1948-1974", "1975-1977", "1978-1982", "1983-1988", "1989-2000",
            "2001-2005", "2006-2012", "2013-2021", "apr√®s 2021"
        ],
        "type_batiment": ["appartement", "maison"],
        "type_installation_chauffage": ["individuel", "collectif","mixte (collectif-individuel)"],
        "type_installation_ecs": ["individuel", "collectif", "mixte (collectif-individuel)","INCONNU"],
        "zone_clim_simple": ["H1", "H2", "H3"],
        "type_energie_principale_chauffage": [
            "Bois ‚Äì B√ªches",
            "Bois ‚Äì Granul√©s (pellets) ou briquettes",
            "Bois ‚Äì Plaquettes d‚Äôindustrie",
            "Bois ‚Äì Plaquettes foresti√®res",
            "Butane",
            "Charbon",
            "Fioul domestique",
            "GPL",
            "Gaz naturel",
            "Propane",
            "R√©seau de Chauffage urbain",
            "√âlectricit√©",
            "√âlectricit√© d'origine renouvelable utilis√©e dans le b√¢timent",
        ],
        "type_emetteur_installation_chauffage_n1": [
            "Autres",
            "Convecteur √©lectrique NFC  NF** et NF***",
            "Panneau rayonnant NFC  NF** et NF***",
            "Radiateur bitube avec robinet thermostatique sur r√©seau individuel eau chaude basse ou moyenne temp√©rature(inf 65¬∞C)",
            "Radiateur bitube avec robinet thermostatique sur r√©seau individuel eau chaude haute temp√©rature(sup ou egal 65¬∞C)",
            "Vide",
            "radiateur √©lectrique NFC  NF** et NF***",
        ],
        "type_energie_generateur_n1_ecs_n1": [
            "√âlectricit√©",
            "Bois ‚Äì B√ªches",
            "Bois ‚Äì Granul√©s (pellets) ou briquettes",
            "Bois ‚Äì Plaquettes d‚Äôindustrie",
            "Bois ‚Äì Plaquettes foresti√®res",
            "Butane",
            "Charbon",
            "Fioul domestique",
            "GPL",
            "Gaz naturel",
            "Propane",
            "R√©seau de Chauffage urbain",
            "√âlectricit√© d'origine renouvelable utilis√©e dans le b√¢timent",
            "Vide",
        ],
        "type_energie_n1": [
            "√âlectricit√©",
            "Bois ‚Äì B√ªches",
            "Bois ‚Äì Granul√©s (pellets) ou briquettes",
            "Bois ‚Äì Plaquettes d‚Äôindustrie",
            "Bois ‚Äì Plaquettes foresti√®res",
            "Butane",
            "Charbon",
            "Fioul domestique",
            "GPL",
            "Gaz naturel",
            "Propane",
            "R√©seau de Chauffage urbain",
            "√âlectricit√© d'origine renouvelable utilis√©e dans le b√¢timent",
        ],
        "type_energie_n2": [
            "AUCUN",
            "Bois ‚Äì B√ªches",
            "Bois ‚Äì Granul√©s (pellets) ou briquettes",
            "Bois ‚Äì Plaquettes d‚Äôindustrie",
            "Bois ‚Äì Plaquettes foresti√®res",
            "Butane",
            "Charbon",
            "Fioul domestique",
            "GPL",
            "Gaz naturel",
            "Propane",
            "R√©seau de Chauffage urbain",
            "√âlectricit√©",
            "√âlectricit√© d'origine renouvelable utilis√©e dans le b√¢timent",
        ],
        "type_energie_principale_ecs": [
            "√âlectricit√©",
            "Butane",
            "Charbon",
            "Fioul domestique",
            "GPL",
            "Gaz naturel",
            "Propane",
            "R√©seau de Chauffage urbain",
            "√âlectricit√© d'origine renouvelable utilis√©e dans le b√¢timent",
            "Bois ‚Äì B√ªches",
            "Bois ‚Äì Granul√©s (pellets) ou briquettes",
            "Bois ‚Äì Plaquettes d‚Äôindustrie",
            "Bois ‚Äì Plaquettes foresti√®res",
            "Non affect√©",
        ],
        "type_generateur_chauffage_principal": [
            "Chaudi√®re gaz √† condensation 2001-2015",
            "Chaudi√®re gaz √† condensation apr√®s 2015",
            "Convecteur √©lectrique NFC  NF** et NF***",
            "Panneau rayonnant √©lectrique NFC  NF** et NF***",
            "Radiateur √©lectrique √† accumulation",
            "R√©seau de chaleur isol√©",
            "Autres",
            "Vide",
        ],
        "type_generateur_chauffage_principal_ecs": [
            "Ballon √©lectrique √† accumulation vertical Autres ou inconnue",
            "Ballon √©lectrique √† accumulation vertical Cat√©gorie B ou 2 √©toiles",
            "Chaudi√®re gaz √† condensation 2001-2015",
            "Chaudi√®re gaz √† condensation apr√®s 2015",
            "Autres",
            "Vide",
        ],
        # ‚ö†Ô∏è Ces 4-l√† sont dans col_oe => ordinal_map 
        "qualite_isolation_enveloppe": ["insuffisante", "moyenne", "bonne", "tr√®s bonne"],
        "qualite_isolation_murs": ["insuffisante", "moyenne", "bonne", "tr√®s bonne"],
        "qualite_isolation_plancher_haut": ["insuffisante", "moyenne", "bonne", "tr√®s bonne"],
        "classe_inertie_batiment": ["L√©g√®re", "Moyenne", "Lourde", "Tr√®s lourde"],
    }

    st.markdown(
        """
        Remplissez les caract√©ristiques du logement.
        """
    )

    with st.form("form_simulation"):
        with st.container(border=True):
            st.markdown("**Caract√©ristiques du logement**")
            c1, c2, c3 = st.columns(3)
            with c1:                   
                type_batiment = st.selectbox("Type b√¢timent", form_options["type_batiment"], index=1)                                  
                classe_altitude = st.selectbox("Classe altitude", form_options["classe_altitude"], index=0)
            with c2:
                surface_habitable_logement = st.number_input(
                "Surface habitable (m¬≤)",
                min_value=9.0,
                max_value=500.0,
                value=80.0,
                step=1.0
                )
                zone_clim_simple = st.selectbox("Zone climatique", form_options["zone_clim_simple"])
            with c3:
                periode_construction = st.selectbox("P√©riode construction", form_options["periode_construction"])                 
        with st.container(border=True):
            st.markdown("**Energies chauffage et ECS**")
            e1, e2, e3 = st.columns(3)
            with e1:
                type_installation_chauffage = st.selectbox("Installation chauffage", form_options["type_installation_chauffage"], index=0)
                type_generateur_chauffage_principal = st.selectbox("G√©n√©rateur chauffage principal", form_options["type_generateur_chauffage_principal"])
                type_installation_ecs = st.selectbox("Installation ECS", form_options["type_installation_ecs"], index=0)

            with e2:
                valeur_par_defaut_type_energie_principale_chauffage = "√âlectricit√©"
                # On cherche l'index dans la liste
                default_index_type_energie_principale_chauffage = form_options["type_energie_principale_chauffage"].index(valeur_par_defaut_type_energie_principale_chauffage)
                type_energie_principale_chauffage = st.selectbox("√ânergie principale chauffage", form_options["type_energie_principale_chauffage"], index=default_index_type_energie_principale_chauffage)
                type_energie_principale_ecs = st.selectbox("√ânergie principale ECS", form_options["type_energie_principale_ecs"])
            with e3:
                type_emetteur_installation_chauffage_n1 = st.selectbox("√âmetteur chauffage", form_options["type_emetteur_installation_chauffage_n1"])
                type_generateur_chauffage_principal_ecs = st.selectbox("G√©n√©rateur chauffage principal ECS", form_options["type_generateur_chauffage_principal_ecs"])
        with st.container(border=True):
            st.markdown("**Isolation / inertie**")
            i1, i2 = st.columns(2)
            with i1:
                qualite_isolation_enveloppe = st.selectbox("Qualit√© isolation enveloppe", form_options["qualite_isolation_enveloppe"], index=1)
                qualite_isolation_murs = st.selectbox("Qualit√© isolation murs", form_options["qualite_isolation_murs"], index=1)
            with i2:
                qualite_isolation_plancher_haut = st.selectbox("Qualit√© isolation plancher haut", form_options["qualite_isolation_plancher_haut"], index=1)
                classe_inertie_batiment = st.selectbox("Classe inertie b√¢timent", form_options["classe_inertie_batiment"], index=1)
        with st.expander("√ânergies secondaires (optionnel)"):
            ee1, ee2 = st.columns(2)
            with ee1:
                type_energie_n1 = st.selectbox("Type √©nergie n¬∞1", form_options["type_energie_n1"])
                type_energie_generateur_n1_ecs_n1 = st.selectbox("√ânergie g√©n√©rateur n¬∞1 ECS", form_options["type_energie_generateur_n1_ecs_n1"])
            with ee2:
                type_energie_n2 = st.selectbox("Type √©nergie n¬∞2", form_options["type_energie_n2"])
        submitted = st.form_submit_button("üöÄ Lancer la pr√©diction", use_container_width=True)

    if submitted:
        # üî• IMPORTANT :
        # Le preprocess utilise cat_selector/num_selector => il attend les colonnes brutes pr√©sentes √† l'entra√Ænement.
        # Ici on remplit au minimum celles dont tu as les modalit√©s + les 4 ordinales.
        raw_features = {
            "type_batiment": type_batiment,
            "periode_construction": periode_construction,
            "surface_habitable_logement": float(surface_habitable_logement),
            "type_installation_chauffage": type_installation_chauffage,
            "classe_altitude": classe_altitude,
            "type_energie_principale_chauffage": type_energie_principale_chauffage,
            "type_emetteur_installation_chauffage_n1": type_emetteur_installation_chauffage_n1,
            "type_energie_generateur_n1_ecs_n1": type_energie_generateur_n1_ecs_n1,
            "type_energie_n1": type_energie_n1,
            "type_energie_n2": type_energie_n2,
            "type_energie_principale_ecs": type_energie_principale_ecs,
            "type_installation_ecs": type_installation_ecs,
            "type_generateur_chauffage_principal": type_generateur_chauffage_principal,
            "type_generateur_chauffage_principal_ecs": type_generateur_chauffage_principal_ecs,
            "zone_clim_simple": zone_clim_simple,
            # ordinal_map:
            "qualite_isolation_enveloppe": qualite_isolation_enveloppe,
            "qualite_isolation_murs": qualite_isolation_murs,
            "qualite_isolation_plancher_haut": qualite_isolation_plancher_haut,
            "classe_inertie_batiment": classe_inertie_batiment,
        }

        # conso √† partir du modele
        try:
            conso_pred = predict_from_model(preprocess_conso, y_scaler_conso, model_conso, raw_features)
        except Exception as e:
            st.error(
                "Erreur pendant la pr√©diction.\n\n"
                f"D√©tail : {e}"
            )
            st.stop()

        # GES √† partir du mod√®le :
        try:
            ges_pred = predict_from_model(preprocess_ges, y_scaler_ges, model_ges, raw_features)
        except Exception as e:
            st.error(
                "Erreur pendant la pr√©diction.\n\n"
                f"D√©tail : {e}"
            )
            st.stop()

        classe_finale = get_classe_dpe(conso_pred, ges_pred)

        st.divider()
        st.header("R√©sultats de l'estimation")

        col_res1, col_res2 = st.columns([1, 2])

        with col_res1:
            st.metric("Consommation (Ep)", f"{conso_pred:.0f} kWh/m¬≤/an")
            st.metric("√âmissions (GES)", f"{ges_pred:.0f} kgCO2/m¬≤/an")

            color_map = {
                "A": "#009036", "B": "#53af31", "C": "#c6d300", "D": "#fce600",
                "E": "#fbba00", "F": "#eb6105", "G": "#d40f14",
            }
            st.markdown(
                f"""
                <div style="text-align:center; background-color:{color_map[classe_finale]};
                            padding:10px; border-radius:10px;">
                    <h1 style="color:white; margin:0;">CLASSE {classe_finale}</h1>
                </div>
                """,
                unsafe_allow_html=True,
            )

        with col_res2:
            base_url = "https://www.outils.immo/outils-immo.php"
            params = (
                f"?type=dpe&modele=2021&valeur={int(round(conso_pred))}"
                f"&lettre={classe_finale}&valeurges={int(round(ges_pred))}"
            )
            st.image(base_url + params, width=400)

        st.success("Pr√©diction termin√©e")
        st.warning("""
        **Disclaimer ‚Äì Usage des pr√©dictions**
        
        Les r√©sultats affich√©s par cette application sont issus d‚Äôun mod√®le de
        machine learning entra√Æn√© sur des donn√©es historiques.
        
        Ils sont fournis **√† titre indicatif et p√©dagogique** et ne constituent
        **en aucun cas un Diagnostic de Performance √ânerg√©tique (DPE) officiel**
        au sens r√©glementaire.
        
        Les pr√©dictions doivent √™tre interpr√©t√©es avec prudence, compte tenu des
        limites li√©es √† la qualit√© des donn√©es, aux hypoth√®ses de mod√©lisation
        et √† la g√©n√©ralisation du mod√®le.
        """)
        # ----------------------------
        # Expander d'explicabilit√© (local)
        # ----------------------------
        with st.expander("üîé Explicabilit√© : variables qui influencent la pr√©diction (conso)"):
            try:
                X_scaled_2d, feature_names = get_X_scaled_and_feature_names(preprocess_conso, raw_features)
                
                # base_pred + df_imp via tes helpers d√©j√† ajout√©s
                base_pred, df_imp = local_permutation_importance(
                    model=model_conso,
                    y_scaler=y_scaler_conso,
                    X_scaled_2d=X_scaled_2d,           # shape (1, n_features)
                    feature_names=feature_names,
                    n_repeats=10,
                    mode="zero",                       # marche tr√®s bien en standardis√©
                    random_state=42,
                )

                st.write(f"Consommation pr√©dite (base) : **{base_pred:.1f} kWh/m¬≤/an**")

                # Top 20 features finales (apr√®s OHE)
                st.caption("Top 20 des features finales (apr√®s encodage OneHot + scaling).")
                st.dataframe(df_imp.head(20), use_container_width=True)

                # Optionnel : regroupement plus lisible par variable d'origine
                def group_by_original_column(df_imp: pd.DataFrame, preprocess) -> pd.DataFrame:
                    """
                    Regroupe les importances par colonne d'origine (avant OHE) de fa√ßon robuste.
                    On r√©cup√®re les colonnes r√©ellement vues par le ColumnTransformer (cat + num),
                    puis on matche par pr√©fixe le plus long.
                    """
                    ct = preprocess.named_steps.get("encode_and_scale", None)
                    if ct is None:
                        # fallback : pas de regroupement possible
                        df = df_imp.copy()
                        df["variable"] = "(inconnu)"
                        return df.groupby("variable", as_index=False)["impact_abs_moyen"].sum()

                    # R√©cup√®re les colonnes sources r√©ellement utilis√©es par cat/num
                    used_cols = []
                    for name, transformer, cols in ct.transformers_:
                        if name in ("cat", "num"):
                            # cols peut √™tre un array de noms de colonnes
                            if isinstance(cols, (list, tuple, np.ndarray)):
                                used_cols.extend(list(cols))

                    used_cols = [c for c in used_cols if isinstance(c, str)]
                    used_cols = sorted(set(used_cols), key=len, reverse=True)

                    def infer_base_col(feat_name: str) -> str:
                        # exact match : num features souvent = nom de colonne
                        if feat_name in used_cols:
                            return feat_name

                        # match pr√©fixe long : ohe -> "<col>_<modalit√©>"
                        for col in used_cols:
                            if feat_name.startswith(col + "_"):
                                return col

                        return "(autres)"

                    df = df_imp.copy()
                    df["variable"] = df["feature"].astype(str).apply(infer_base_col)

                    return (
                        df.groupby("variable", as_index=False)["impact_abs_moyen"]
                        .sum()
                        .sort_values("impact_abs_moyen", ascending=False)
                        .reset_index(drop=True)
                    )



                df_group = group_by_original_column(df_imp, preprocess_conso)
                st.caption("Regroupement par variable avant OHE (plus lisible).")
                st.dataframe(df_group.head(20), use_container_width=True)

                # Debug utile pour interop locale
                with st.expander("üß™ Debug (features envoy√©es au mod√®le)"):
                    st.json(raw_features)

            except Exception as e:
                st.warning(
                    "Impossible de calculer l'explicabilit√© pour cette pr√©diction.\n\n"
                    f"D√©tail : {e}"
                )

# ----------------------------
# ROUTER
# ----------------------------
if page == "üèÅ Pr√©sentation":
    page_presentation()
elif page == "üìä Dataviz":
    page_dataviz()
elif page == "üìà R√©sultats d'entra√Ænement":
    page_results()
elif page == "üßÆ Pr√©diction DPE":
    page_simulator()
